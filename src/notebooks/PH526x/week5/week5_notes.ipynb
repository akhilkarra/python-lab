{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Week 5 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.1 Introduction to Statistical Learning\n",
    "\n",
    "Statistical learning is the process of using data to learn about the underlying distribution of a target variable.\n",
    "- supervised learning: the process of learning from data that has a known target variable\n",
    "- unsupervised learning: the process of learning from data that has no target variable\n",
    "\n",
    "Supervised learning matches inputs and outputs, whereas unsupervised learning discovers structure for inputs only.\n",
    "\n",
    "We will learn about the basics of supervised learning in this case study. We will cover both regression (quantitative outcome) and classification problems (qualitative outcome).\n",
    "\n",
    "Loss functions are used to quantify the quality of a model. In regression, the most common loss function is the Mean Squared Error (MSE). In classification, the most common loss function is 0-1 Loss.\n",
    "\n",
    "Say we have a dataset and want to predict the relationship between a predictor variable $X$ and a target variable $Y$. The basics of regression involves finding an average point around a small subset of points in the dataset, and then predicting the value of $Y$ for a new point $X$.\n",
    "\n",
    "Now say our data set has an output of just 0 and 1 (we have a classification problem now). We now want to estimate the outcome of the target variable $Y$ based on the predictor variable $X$ by finding the outcome of $Y$ with the highest probability for the given value of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.2 Generating Example Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data parameters\n",
    "number_of_samples = 100\n",
    "beta_0 = 5\n",
    "beta_1 = 2\n",
    "\n",
    "np.random.seed(1)  # set seed for reproducibility\n",
    "\n",
    "x = 10 * stats.uniform.rvs(\n",
    "    size=number_of_samples\n",
    ")  # generate uniform random numbers between 0 and 10\n",
    "y = (\n",
    "    beta_0 + beta_1 * x + stats.norm.rvs(loc=0, scale=1, size=number_of_samples)\n",
    ")  # generate normal random numbers with mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, \"o\", markersize=5)\n",
    "xx = np.array([0, 10])\n",
    "plt.plot(xx, beta_0 + beta_1 * xx)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(x), np.mean(y))  # Comprehension Check Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.3 Simple Linear Regression\n",
    "\n",
    "A simple linear regression model involves assuming the relationship between the predictor variable $X$ and the target variable $Y$ is linear; i.e. $$Y = \\beta_0 + \\beta_1 X + \\epsilon.$$\n",
    "\n",
    "We can use training data to estimate the parameters $\\beta_0$ and $\\beta_1$, and then we can predict future values of $Y$ based on  new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehension Check Question\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "n = 100\n",
    "beta_0 = 5\n",
    "beta_1 = 2\n",
    "np.random.seed(1)\n",
    "x = 10 * ss.uniform.rvs(size=n)\n",
    "y = beta_0 + beta_1 * x + ss.norm.rvs(loc=0, scale=1, size=n)\n",
    "\n",
    "\n",
    "def compute_rss(y_estimate, y):\n",
    "    return sum(np.power(y - y_estimate, 2))\n",
    "\n",
    "\n",
    "def estimate_y(x, b_0, b_1):\n",
    "    return b_0 + b_1 * x\n",
    "\n",
    "\n",
    "rss = compute_rss(estimate_y(x, beta_0, beta_1), y)\n",
    "\n",
    "rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.4 Least Square Regression in Code\n",
    "\n",
    "Our method of regression will involve matrix calculus. We will assume we know the true value of $\\beta_0$ and we are trying to find the value of $\\beta_1$ that minimizes the RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rss = []  # initialize list to store rss values\n",
    "slopes = np.arange(-10, 15, 0.01)  # generate a range of slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for (\n",
    "    slope\n",
    ") in slopes:  # compute the rss value of each slope and store it in the rss llist\n",
    "    rss.append(np.sum((y - beta_0 - slope * x) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rss[0:10]  # check the first 10 values of the rss list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index_of_min_rss = np.argmin(rss)  # find the index of the minimum rss value\n",
    "index_of_min_rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "slopes[index_of_min_rss]  # find the slope that minimizes the rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us plot a generated slopes vs. corresponding RSS values graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(slopes, rss)\n",
    "plt.xlabel(\"slopes\")\n",
    "plt.ylabel(\"rss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 5.1.5 Simple Linear Regression in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.OLS(y, x)\n",
    "estimate = model.fit()\n",
    "print(estimate.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model generated is actually a linear model with no intercept term; that's why we have an artificially large slope value when we know the true slope to be $2$ from our specified randomly generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(x))\n",
    "estimate = model.fit()\n",
    "print(estimate.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.6 Multiple Linear Regression\n",
    "\n",
    "Here, we are trying to find the predicted value of a target variable $Y$ from several predictor variables $X$.\n",
    "\n",
    "The coefficient of each predictor variable is read as the change in $Y$ with a unit change of the predictor variable keeping all other predictor variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.7 Using scikit-learn for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import sklearn\n",
    "\n",
    "np.random.seed(1)  # set the random seed to 1 for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us generate a linear regression of a randomly generated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 500  # number of data points generated in model\n",
    "\n",
    "# set coefficients of the multiple linear regression model\n",
    "beta_0 = 5\n",
    "beta_1 = 2\n",
    "beta_2 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1 = 10 * ss.uniform.rvs(size=n)  # generate x1 values\n",
    "x2 = 10 * ss.uniform.rvs(size=n)  # generate x2 values\n",
    "y = (\n",
    "    beta_0 + beta_1 * x1 + beta_2 * x2 + ss.norm.rvs(loc=0, scale=1, size=n)\n",
    ")  # generate y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can actually stack x1 and x2 into a single matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.stack((x1, x2), axis=1)  # create the predictor matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let us plot the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(X[:, 0], X[:, 1], y, c=y)\n",
    "ax.set_xlabel(\"x1\")\n",
    "ax.set_ylabel(\"x2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us create the regression fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression(fit_intercept=True)\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us now find some aspects of the regression fit, starting with the value of $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let us find the values of $\\beta_1$ and $\\beta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us predict the value of $Y$ given a new predictor set $X =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "4\n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm.predict(np.array([2, 4]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, let us figure how close the regression fit is to the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1.8 Assessing Model Accuracy\n",
    "\n",
    "Most common way to measure regression model accuracy is the Mean Square Error (MSE).\n",
    "\n",
    "Most common way to measure classification model accuracy is the Misclassification Rate (MCR) or Test Error Rate (TER); how often the model miscategorizes data from a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=1\n",
    ")  # random state fixed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm.score(X_test, y_test)  # Test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The interesting thing to note is we want our model's accuracy to be APPROPRIATE for the data we are using; if it is too precise, we can overfit the data and cause the model to become overly sensitive to noise. However, if the model is too inaccurate, we can underfit the data and cause the model to not be able to sufficiently generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.1 Generating Example Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img alt=\"image\" src=\"../../../../../Desktop/Screen Shot 2022-05-16 at 7.55.11 PM.png\"/>\n",
    "\n",
    "The problem set up is that we have data generated from two 2D normal distributions. We want to classify from which distribution the data points are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 1  # step size in the mesh\n",
    "sd = 1  # standard deviation of the distributions\n",
    "n = 50  # number of data points generated in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(n: int, h: float, sd1: float, sd2: float) -> tuple:\n",
    "    x1 = ss.norm.rvs(-h, sd1, n)  # generate x1 values\n",
    "    y1 = ss.norm.rvs(0, sd1, n)  # generate y1 values\n",
    "\n",
    "    x2 = ss.norm.rvs(h, sd2, n)  # generate x2 values\n",
    "    y2 = ss.norm.rvs(0, sd2, n)  # generate y2 values\n",
    "\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1.5, 1, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(x1: float, x2: float, y1: float, y2: float) -> None:\n",
    "    plt.figure()\n",
    "    plt.plot(x1, y1, \"o\", ms=2, label=\"Class 1\")\n",
    "    plt.plot(x2, y2, \"o\", ms=2, label=\"Class 2\")\n",
    "    plt.legend(\"upper right\")\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The orange cloud is bigger because it has a higher standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehension Check Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 0, 1, 1)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1, 2, 2.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 10, 100, 100)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 20, 0.5, 0.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.2 Logistic Regression\n",
    "\n",
    "It's a binary classifier. Also called conditional class probabilities.\n",
    "\n",
    "This is a classifier that classifies data points based on conditionals on the values of the predictor variables.\n",
    "\n",
    "The method of calculating the classifier involves finding the probability of a data point belonging to each class by using linear regression techniques and logarithms to restrict the value of probabilities to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.3 Logistic Regression in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((np.vstack((x1, y1)).T, np.vstack((x2, y2)).T))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y = np.hstack((np.repeat(1, n), np.repeat(2, n)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)  # Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  # Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Probability of a new point in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Determining which class a new point will be allocated to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.4 Computing Predictive Probabilities Across the Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_probs(ax, clf, class_no):\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-5, 5, 0.1), np.arange(-5, 5, 0.1))\n",
    "    probs = clf.predict_proba(np.stack((xx1.ravel(), xx2.ravel()), axis=1))\n",
    "    Z = probs[:, class_no]\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    CS = ax.contourf(xx1, xx2, Z)\n",
    "    cbar = plt.colorbar(CS)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "ax = plt.subplot(211)\n",
    "plot_probs(ax, clf, 0)\n",
    "plt.title(\"Pred. prob for class 1\")\n",
    "ax = plt.subplot(212)\n",
    "plot_probs(ax, clf, 1)\n",
    "plt.title(\"Pred. prob for class 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.1 Generating Example Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img alt=\"image\" src=\"../../../../../Desktop/Screen Shot 2022-05-16 at 7.55.11 PM.png\"/>\n",
    "\n",
    "The problem set up is that we have data generated from two 2D normal distributions. We want to classify from which distribution the data points are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 1  # step size in the mesh\n",
    "sd = 1  # standard deviation of the distributions\n",
    "n = 50  # number of data points generated in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(n: int, h: float, sd1: float, sd2: float) -> tuple:\n",
    "    x1 = ss.norm.rvs(-h, sd1, n)  # generate x1 values\n",
    "    y1 = ss.norm.rvs(0, sd1, n)  # generate y1 values\n",
    "\n",
    "    x2 = ss.norm.rvs(h, sd2, n)  # generate x2 values\n",
    "    y2 = ss.norm.rvs(0, sd2, n)  # generate y2 values\n",
    "\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1.5, 1, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(x1: float, x2: float, y1: float, y2: float) -> None:\n",
    "    plt.figure()\n",
    "    plt.plot(x1, y1, \"o\", ms=2, label=\"Class 1\")\n",
    "    plt.plot(x2, y2, \"o\", ms=2, label=\"Class 2\")\n",
    "    plt.legend(\"upper right\")\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The orange cloud is bigger because it has a higher standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehension Check Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 0, 1, 1)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1, 2, 2.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 10, 100, 100)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 20, 0.5, 0.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.2 Logistic Regression\n",
    "\n",
    "It's a binary classifier. Also called conditional class probabilities.\n",
    "\n",
    "This is a classifier that classifies data points based on conditionals on the values of the predictor variables.\n",
    "\n",
    "The method of calculating the classifier involves finding the probability of a data point belonging to each class by using linear regression techniques and logarithms to restrict the value of probabilities to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.3 Logistic Regression in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((np.vstack((x1, y1)).T, np.vstack((x2, y2)).T))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y = np.hstack((np.repeat(1, n), np.repeat(2, n)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)  # Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  # Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Probability of a new point in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Determining which class a new point will be allocated to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.4 Computing Predictive Probabilities Across the Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_probs(ax, clf, class_no):\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-5, 5, 0.1), np.arange(-5, 5, 0.1))\n",
    "    probs = clf.predict_proba(np.stack((xx1.ravel(), xx2.ravel()), axis=1))\n",
    "    Z = probs[:, class_no]\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    CS = ax.contourf(xx1, xx2, Z)\n",
    "    cbar = plt.colorbar(CS)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "ax = plt.subplot(211)\n",
    "plot_probs(ax, clf, 0)\n",
    "plt.title(\"Pred. prob for class 1\")\n",
    "ax = plt.subplot(212)\n",
    "plot_probs(ax, clf, 1)\n",
    "plt.title(\"Pred. prob for class 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5.3.1 Tree-Based Models for Regression and Classification\n",
    "\n",
    "We break the predictor space into smaller and smaller regions until we hit a stopping condition. For a particular test observation, we look at the region of the predictor space that is most likely to contain the test observation. For regression models, we find the mean of the outcomes in this region. For classification models, we find the mode of the outcomes in this region.\n",
    "\n",
    "We make divisions by looking for maximally homogeneous regions of the predictor space. We keep making division until we get the lowest value of the loss function: RSS for regression and either Gini index or Cross entropy for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.3.2 Random Forest Predictions\n",
    "\n",
    "A random forest prediction is made from aggregating several trees and making each tree different by bagging each data set with replacement and also constricting the predictor variables considered with each cut. We then take the mean for regression or mode for classification of the predictions from each tree.\n",
    "\n",
    "Copy-pasted from the notes:\n",
    "\n",
    "One of the great features of the `sklearn` library is that there is a consistent framework for the workflow needed to use different statistical models.\n",
    "\n",
    "To do random forest regression, you use the following import:\n",
    "\n",
    "`from sklearn.ensemble import RandomForestRegressor`\n",
    "\n",
    "To do random forest classification, you use the following import:\n",
    "\n",
    "`from sklearn.ensemble import RandomForestClassifier`\n",
    "\n",
    "After importing the relevant model, everything proceeds in the same way as for linear and logistic regression as shown in the previous videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.1 Generating Example Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img alt=\"image\" src=\"../../../../../Desktop/Screen Shot 2022-05-16 at 7.55.11 PM.png\"/>\n",
    "\n",
    "The problem set up is that we have data generated from two 2D normal distributions. We want to classify from which distribution the data points are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 1  # step size in the mesh\n",
    "sd = 1  # standard deviation of the distributions\n",
    "n = 50  # number of data points generated in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(n: int, h: float, sd1: float, sd2: float) -> tuple:\n",
    "    x1 = ss.norm.rvs(-h, sd1, n)  # generate x1 values\n",
    "    y1 = ss.norm.rvs(0, sd1, n)  # generate y1 values\n",
    "\n",
    "    x2 = ss.norm.rvs(h, sd2, n)  # generate x2 values\n",
    "    y2 = ss.norm.rvs(0, sd2, n)  # generate y2 values\n",
    "\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1.5, 1, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(x1: float, x2: float, y1: float, y2: float) -> None:\n",
    "    plt.figure()\n",
    "    plt.plot(x1, y1, \"o\", ms=2, label=\"Class 1\")\n",
    "    plt.plot(x2, y2, \"o\", ms=2, label=\"Class 2\")\n",
    "    plt.legend(\"upper right\")\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The orange cloud is bigger because it has a higher standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehension Check Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 0, 1, 1)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1, 2, 2.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 10, 100, 100)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 20, 0.5, 0.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.2 Logistic Regression\n",
    "\n",
    "It's a binary classifier. Also called conditional class probabilities.\n",
    "\n",
    "This is a classifier that classifies data points based on conditionals on the values of the predictor variables.\n",
    "\n",
    "The method of calculating the classifier involves finding the probability of a data point belonging to each class by using linear regression techniques and logarithms to restrict the value of probabilities to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.3 Logistic Regression in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((np.vstack((x1, y1)).T, np.vstack((x2, y2)).T))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y = np.hstack((np.repeat(1, n), np.repeat(2, n)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)  # Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  # Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Probability of a new point in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Determining which class a new point will be allocated to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.4 Computing Predictive Probabilities Across the Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_probs(ax, clf, class_no):\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-5, 5, 0.1), np.arange(-5, 5, 0.1))\n",
    "    probs = clf.predict_proba(np.stack((xx1.ravel(), xx2.ravel()), axis=1))\n",
    "    Z = probs[:, class_no]\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    CS = ax.contourf(xx1, xx2, Z)\n",
    "    cbar = plt.colorbar(CS)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "ax = plt.subplot(211)\n",
    "plot_probs(ax, clf, 0)\n",
    "plt.title(\"Pred. prob for class 1\")\n",
    "ax = plt.subplot(212)\n",
    "plot_probs(ax, clf, 1)\n",
    "plt.title(\"Pred. prob for class 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5.3.1 Tree-Based Models for Regression and Classification\n",
    "\n",
    "We break the predictor space into smaller and smaller regions until we hit a stopping condition. For a particular test observation, we look at the region of the predictor space that is most likely to contain the test observation. For regression models, we find the mean of the outcomes in this region. For classification models, we find the mode of the outcomes in this region.\n",
    "\n",
    "We make divisions by looking for maximally homogeneous regions of the predictor space. We keep making division until we get the lowest value of the loss function: RSS for regression and either Gini index or Cross entropy for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.3.2 Random Forest Predictions\n",
    "\n",
    "A random forest prediction is made from aggregating several trees and making each tree different by bagging each data set with replacement and also constricting the predictor variables considered with each cut. We then take the mean for regression or mode for classification of the predictions from each tree.\n",
    "\n",
    "Copy-pasted from the notes:\n",
    "\n",
    "One of the great features of the `sklearn` library is that there is a consistent framework for the workflow needed to use different statistical models.\n",
    "\n",
    "To do random forest regression, you use the following import:\n",
    "\n",
    "`from sklearn.ensemble import RandomForestRegressor`\n",
    "\n",
    "To do random forest classification, you use the following import:\n",
    "\n",
    "`from sklearn.ensemble import RandomForestClassifier`\n",
    "\n",
    "After importing the relevant model, everything proceeds in the same way as for linear and logistic regression as shown in the previous videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.1 Generating Example Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img alt=\"image\" src=\"../../../../../Desktop/Screen Shot 2022-05-16 at 7.55.11 PM.png\"/>\n",
    "\n",
    "The problem set up is that we have data generated from two 2D normal distributions. We want to classify from which distribution the data points are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h = 1  # step size in the mesh\n",
    "sd = 1  # standard deviation of the distributions\n",
    "n = 50  # number of data points generated in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(n: int, h: float, sd1: float, sd2: float) -> tuple:\n",
    "    x1 = ss.norm.rvs(-h, sd1, n)  # generate x1 values\n",
    "    y1 = ss.norm.rvs(0, sd1, n)  # generate y1 values\n",
    "\n",
    "    x2 = ss.norm.rvs(h, sd2, n)  # generate x2 values\n",
    "    y2 = ss.norm.rvs(0, sd2, n)  # generate y2 values\n",
    "\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1.5, 1, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(x1: float, x2: float, y1: float, y2: float) -> None:\n",
    "    plt.figure()\n",
    "    plt.plot(x1, y1, \"o\", ms=2, label=\"Class 1\")\n",
    "    plt.plot(x2, y2, \"o\", ms=2, label=\"Class 2\")\n",
    "    plt.legend(\"upper right\")\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The orange cloud is bigger because it has a higher standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehension Check Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 0, 1, 1)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 1, 2, 2.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 10, 100, 100)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = generate_data(1000, 20, 0.5, 0.5)\n",
    "plot_data(x1, x2, y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.2 Logistic Regression\n",
    "\n",
    "It's a binary classifier. Also called conditional class probabilities.\n",
    "\n",
    "This is a classifier that classifies data points based on conditionals on the values of the predictor variables.\n",
    "\n",
    "The method of calculating the classifier involves finding the probability of a data point belonging to each class by using linear regression techniques and logarithms to restrict the value of probabilities to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.3 Logistic Regression in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((np.vstack((x1, y1)).T, np.vstack((x2, y2)).T))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y = np.hstack((np.repeat(1, n), np.repeat(2, n)))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)  # Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)  # Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Probability of a new point in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(\n",
    "    np.array([-2, 0]).reshape(1, -1)\n",
    ")  # Determining which class a new point will be allocated to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2.4 Computing Predictive Probabilities Across the Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_probs(ax, clf, class_no):\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-5, 5, 0.1), np.arange(-5, 5, 0.1))\n",
    "    probs = clf.predict_proba(np.stack((xx1.ravel(), xx2.ravel()), axis=1))\n",
    "    Z = probs[:, class_no]\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    CS = ax.contourf(xx1, xx2, Z)\n",
    "    cbar = plt.colorbar(CS)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "ax = plt.subplot(211)\n",
    "plot_probs(ax, clf, 0)\n",
    "plt.title(\"Pred. prob for class 1\")\n",
    "ax = plt.subplot(212)\n",
    "plot_probs(ax, clf, 1)\n",
    "plt.title(\"Pred. prob for class 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5.3.1 Tree-Based Models for Regression and Classification\n",
    "\n",
    "We break the predictor space into smaller and smaller regions until we hit a stopping condition. For a particular test observation, we look at the region of the predictor space that is most likely to contain the test observation. For regression models, we find the mean of the outcomes in this region. For classification models, we find the mode of the outcomes in this region.\n",
    "\n",
    "We make divisions by looking for maximally homogeneous regions of the predictor space. We keep making division until we get the lowest value of the loss function: RSS for regression and either Gini index or Cross entropy for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.3.2 Random Forest Predictions\n",
    "\n",
    "A random forest prediction is made from aggregating several trees and making each tree different by bagging each data set with replacement and also constricting the predictor variables considered with each cut. We then take the mean for regression or mode for classification of the predictions from each tree.\n",
    "\n",
    "Copy-pasted from the notes:\n",
    "\n",
    "One of the great features of the `sklearn` library is that there is a consistent framework for the workflow needed to use different statistical models.\n",
    "\n",
    "To do random forest regression, you use the following import:\n",
    "\n",
    "`from sklearn.ensemble import RandomForestRegressor`\n",
    "\n",
    "To do random forest classification, you use the following import:\n",
    "\n",
    "`from sklearn.ensemble import RandomForestClassifier`\n",
    "\n",
    "After importing the relevant model, everything proceeds in the same way as for linear and logistic regression as shown in the previous videos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
